apiVersion: batch/v1
kind: CronJob
metadata:
  name: metallb-health-check
  namespace: metallb-system
spec:
  schedule: "*/30 * * * *"  # Every 30 minutes
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: metallb-health-checker
          containers:
          - name: health-check
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              # Check if services are responding
              FAILED=0
              
              # Test critical services
              if ! curl -s -o /dev/null -w "%{http_code}" --connect-timeout 5 http://homepage.homepage.svc.cluster.local > /dev/null 2>&1; then
                echo "Homepage service not responding internally"
                FAILED=1
              fi
              
              if [ $FAILED -eq 1 ]; then
                echo "Services failing, restarting MetalLB speakers..."
                kubectl delete pod -l component=speaker -n metallb-system
              else
                echo "All services healthy"
              fi
          restartPolicy: OnFailure
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metallb-health-checker
  namespace: metallb-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: metallb-health-checker
  namespace: metallb-system
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metallb-health-checker
  namespace: metallb-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: metallb-health-checker
subjects:
- kind: ServiceAccount
  name: metallb-health-checker
  namespace: metallb-system